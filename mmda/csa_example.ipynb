{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mmda.utils.cca_class import NormalizedCCA\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "# ruff: noqa: ERA001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit the CCA model. CCA dimension: 700\n",
      "Train data shape: (1500, 1024), (1500, 768)\n",
      "Test data shape: (500, 700), (500, 700)\n",
      "Save the CCA model to /nas/pohan/datasets/COSMOS/embeddings/test_cca_model.pkl.\n"
     ]
    }
   ],
   "source": [
    "# As cca_zoo does not have the normalized (zero-meaned) the input embeddigns of CCA, we re-wrote a NormalizedCCA class\n",
    "cca = NormalizedCCA() # initialize the CCA class\n",
    "\n",
    "# Due to the rank constraint of CCA, the number of training samples should be larger\n",
    "# than the number of dimensions and sim_dim (see line 15).\n",
    "# The number of samples should be the same for both modalities\n",
    "train_emb_modal1 = np.random.rand(1500, 1024) # 1500 samples, 1024 dimensions\n",
    "train_emb_modal2 = np.random.rand(1500, 768) # 1500 samples, 768 dimensions\n",
    "\n",
    "test_emb_modal1 = np.random.rand(500, 1024) # 500 samples, 1024 dimensions\n",
    "test_emb_modal2 = np.random.rand(500, 768) # 500 samples, 768 dimensions\n",
    "\n",
    "cfg = OmegaConf.load(\"../config/main.yaml\")\n",
    "cfg_dataset = cfg[\"cosmos\"] # we use the COSMOS dataset as an example\n",
    "# cfg_dataset should have (from config/main.yaml):\n",
    "#   sim_dim: 700 # dimension of the similarity score and the CCA transformation\n",
    "#   equal_weights: False # whether to use equal weights for the similarity score or weighted cosine similarity\n",
    "#   img_encoder: \"dino\" # the image encoder to load the image embeddings, not used in the CCA module\n",
    "#   text_encoder: \"gtr\" # the text encoder to load the text embeddings, not used in the CCA module\n",
    "#   paths:\n",
    "#     dataset_path: \"/nas/pohan/datasets/COSMOS/\"\n",
    "#     save_path: ${cosmos.paths.dataset_path}embeddings/\n",
    "#     plots_path: ${repo_root}plots/COSMOS/\n",
    "\n",
    "\n",
    "cca.fit_transform_train_data(cfg_dataset, train_emb_modal1, train_emb_modal2)\n",
    "print(f\"Fit the CCA model. CCA dimension: {cfg_dataset.sim_dim}\")\n",
    "print(f\"Train data shape: {train_emb_modal1.shape}, {train_emb_modal2.shape}\")\n",
    "\n",
    "# transform the test data\n",
    "test_emb_modal1, test_emb_modal2 = cca.transform_data(test_emb_modal1, test_emb_modal2)\n",
    "print(f\"Test data shape: {test_emb_modal1.shape}, {test_emb_modal2.shape}\")\n",
    "\n",
    "# save the CCA model\n",
    "cca_save_path = Path(cfg_dataset.paths.save_path) / \"test_cca_model.pkl\"\n",
    "cca.save_model(cca_save_path)  # save the class object for later use\n",
    "print(f\"Save the CCA model to {cca_save_path}.\")\n",
    "\n",
    "# load the CCA model\n",
    "# cca.load_model(cca_save_path)\n",
    "# print(f\"Load the CCA model from {cca_save_path}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmda-8NF0PIEs-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
